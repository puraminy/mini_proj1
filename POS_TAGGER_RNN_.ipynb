{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_CA4_RNN .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puraminy/mini_proj1/blob/master/POS_TAGGER_RNN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjLb6SBK2ouz",
        "colab_type": "code",
        "outputId": "2457c6bb-1db4-41f9-fae8-4cc52b082c66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data_path = 'gdrive/My Drive/NLP-CA4'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Na58sxD4a41",
        "colab_type": "code",
        "outputId": "abcc7a90-c9ac-408b-9cf5-d02d58b969ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"NLP_CA4_RNN .ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/181q3MmuRBQgMva2FXWVHoM0l0aqiiPwr\n",
        "\"\"\"\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "#data_path = '.'\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"NLP_CA4_2 .ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/181q3MmuRBQgMva2FXWVHoM0l0aqiiPwr\n",
        "\"\"\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "import numpy as np\n",
        "#data_path = '.'\n",
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)\n",
        "\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        "\n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "        \n",
        "def extract_sentences(wordList):\n",
        "  nonBreak_delims = ['،','؛',':']\n",
        "  sentences = []\n",
        "  sentences_tags = []\n",
        "  sent = []\n",
        "  sent_tag = []  \n",
        "  for line in wordList:\n",
        "    tag = line[-1]\n",
        "    word = ''.join(line[:-1])\n",
        "    if tag == \"DELM\" and word not in nonBreak_delims:\n",
        "      if len(sent) > 0:\n",
        "        sentences.append(sent)\n",
        "        sentences_tags.append(sent_tag)\n",
        "      sent = []\n",
        "      sent_tag = []\n",
        "    else:\n",
        "      sent.append(word)\n",
        "      sent_tag.append(tag)\n",
        "  return sentences, sentences_tags\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_data(train_sentences, train_tags):\n",
        "      words, tags = set([]), set([])\n",
        "      for s in train_sentences:\n",
        "          for w in s:\n",
        "              words.add(w.lower())\n",
        "\n",
        "      for ts in train_tags:\n",
        "          for t in ts:\n",
        "              tags.add(t)\n",
        "\n",
        "      word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "      word2index['-PAD-'] = 0  # The special value used for padding\n",
        "      word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        "\n",
        "      tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "      tag2index['-PAD-'] = 0  # The special value used to padding\n",
        "      tag2index['-OOT-'] = -1 \n",
        "\n",
        "      sentences_X, tags_Y= [], []\n",
        "\n",
        "      for s in train_sentences:\n",
        "          s_int = []\n",
        "          for w in s:\n",
        "              try:\n",
        "                  s_int.append(word2index[w.lower()])\n",
        "              except KeyError:\n",
        "                  s_int.append(word2index['-OOV-'])\n",
        "\n",
        "          sentences_X.append(s_int)\n",
        "\n",
        "      for s in train_tags:\n",
        "          tags_y = []\n",
        "          for t in s:\n",
        "              try:\n",
        "                 tags_y.append(tag2index[t])\n",
        "              except KeyError:\n",
        "                  tags_y.append(tag2index['-OOT-'])\n",
        "          tags_Y.append(tags_y)\n",
        "\n",
        "      MAX_LENGTH = len(max(sentences_X, key=len))\n",
        "      sentences_X = pad_sequences(sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "      tags_Y = pad_sequences(tags_Y, maxlen=MAX_LENGTH, padding='post')\n",
        "      \n",
        "      return sentences_X, tags_Y, word2index, tag2index\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# ============================================= Training and Testing ================================\n",
        "use_saved_model = False\n",
        "train_from = 1\n",
        "train_to = 500000 # \n",
        "model_id = str(train_from)+ \"-\" + str(train_to)\n",
        "\n",
        "if not use_saved_model:\n",
        "    trainFileName = data_path + '/train.txt'\n",
        "    trainFile = open(trainFileName,'r', encoding=\"utf-8\")\n",
        "    testFile = open(data_path + '/test.txt','w', encoding=\"utf-8\")\n",
        "    wordList = []\n",
        "    EPOCHS = 6\n",
        "    test_from = 1\n",
        "    test_to = 1000\n",
        "    i = 0\n",
        "    for lines in trainFile:\n",
        "        i+=1\n",
        "        if i in range(test_from, test_to):\n",
        "            testFile.writelines(lines)\n",
        "        elif i in range(train_from, train_to):\n",
        "            wordList.append(lines.split())\n",
        "        else:\n",
        "          break\n",
        "    trainFile.close()\n",
        "    testFile.close()\n",
        "\n",
        "    sentences, sentence_tags =extract_sentences(wordList)\n",
        "    print(\"Sample sentencs\")\n",
        "    for k in range(2):\n",
        "      print(sentences[k])\n",
        "      print(sentence_tags[k])\n",
        "\n",
        "    sentences_X, tags_Y, word2index, tag2index = prepare_data(sentences, sentence_tags)\n",
        "    MAX_LENGTH = len(max(sentences_X, key=len))\n",
        "    # (train_sentences, test_sentences, train_tags, test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)    \n",
        "    kFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
        "    print(\"X shape:\", sentences_X.shape)\n",
        "    print(\"Y shape:\", tags_Y.shape)\n",
        "    cat_train_y = to_categorical(tags_Y, len(tag2index))\n",
        "    print(\"Y shape:\", cat_train_y.shape)\n",
        "    \n",
        "    # Saving word indexes to load them later without creating them again\n",
        "    wordIndexFilename = data_path + \"/word2index-\" + model_id + \".txt\"\n",
        "    print(\"Saving word indexs \", wordIndexFilename)\n",
        "    tagIndexlFilename = data_path + \"/tag2index-\" + model_id + \".txt\"\n",
        "    print(\"Saving word indexs \", tagIndexlFilename)\n",
        "    with open(wordIndexFilename, \"wb\") as fp:   #Pickling\n",
        "        pickle.dump(word2index, fp)\n",
        "    with open(tagIndexlFilename, \"wb\") as fp:   #Pickling\n",
        "        pickle.dump(tag2index, fp) \n",
        "    hist = []\n",
        "    fold = 0\n",
        "    for train, test in kFold.split(sentences_X, tags_Y.argmax(1)):\n",
        "        fold += 1\n",
        "        print(\"Fold:\", fold)\n",
        "        print(\"Preparing data ....\")\n",
        "        # ================================================== MODEL ============================\n",
        "        MAX_LENGTH = len(max(sentences_X[train], key=len))\n",
        "        print(\"Training and testing ....\")\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "        model.add(Embedding(len(word2index), 128))\n",
        "        model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "        model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "        model.add(Activation('softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=Adam(0.001),\n",
        "                      metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "\n",
        "        model.summary()\n",
        "        cat_tags_y = to_categorical(tags_Y, len(tag2index))\n",
        "        model.fit(sentences_X[train], cat_tags_y[train],  \n",
        "                  batch_size=128, epochs=EPOCHS)\n",
        "        scores = model.evaluate(sentences_X[test], cat_tags_y[test])\n",
        "        print(f\"Validation score: {model.metrics_names}:\", scores)\n",
        "        savedModelFilename = data_path + \"/model-\" + model_id+ \"-fold\" + str(fold) + \".h5\"\n",
        "        print(\"Saving model in \", savedModelFilename)\n",
        "        model.save(savedModelFilename)\n",
        "        hist.append(scores)\n",
        "        with open(data_path + \"/hist-\"+model_id+ \".txt\", \"wb\") as fp:   #pickling\n",
        "            pickle.dump(hist, fp)  \n",
        "        \n",
        "    print(hist)\n",
        "    savedModelFilename = data_path + \"/model-\"+model_id+\".h5\"\n",
        "    print(\"Saving model in \", savedModelFilename)\n",
        "    model.save(savedModelFilename)\n",
        "else:\n",
        "  with open(data_path + \"/word2index-\" + model_id + \".txt\", \"rb\") as fp:   # Unpickling\n",
        "      word2index = pickle.load(fp)  \n",
        "  with open(data_path + \"/tag2index-\" + model_id + \".txt\", \"rb\") as fp:   # Unpickling\n",
        "      word2index = pickle.load(fp)  \n",
        "  model = load_model(data_path + \"/model-\"+ model_id + \"-fold1.h5\", {\"ignore_accuracy\":ignore_class_accuracy()})\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample sentencs\n",
            "['به', 'تصوير', 'كند', 'و', 'ظاهرا', 'هيچگاه', 'مانع', 'و', 'محدوديتي', 'هم', 'براي', 'ارااه', 'اين', 'ايدهها', 'وجود', 'نداشته', 'است']\n",
            "['P', 'N_SING', 'V_SUB', 'CON', 'ADV_NI', 'ADV_TIME', 'N_SING', 'CON', 'N_SING', 'CON', 'P', 'N_SING', 'DET', 'N_PL', 'N_SING', 'ADJ_INO', 'V_PRE']\n",
            "['بورن', 'هرگاه', 'احساس', 'كند', 'دنياي', 'كاريكاتور', 'جوابگوي', 'او', 'نيست', '،', 'به', 'نقاشي', 'پناه', 'ميبرد', 'و', 'هنگاميكه', 'ميخواهد', 'موجوداتش', 'را', 'به', 'گونهاي', 'زندهتر', 'حس', 'كند', '،', 'به', 'عنوان', 'انيماتور', 'به', 'كار', 'مشغول', 'ميشود']\n",
            "['N_SING', 'ADV_TIME', 'N_SING', 'V_SUB', 'N_SING', 'N_SING', 'N_SING', 'PRO', 'V_PRS', 'DELM', 'P', 'N_SING', 'N_SING', 'V_PRS', 'CON', 'CON', 'V_PRS', 'N_PL', 'P', 'P', 'N_SING', 'ADJ_CMPR', 'N_SING', 'V_SUB', 'DELM', 'P', 'N_SING', 'N_SING', 'P', 'N_SING', 'ADJ_SIM', 'V_PRS']\n",
            "X shape: (24594, 253)\n",
            "Y shape: (24594, 253)\n",
            "Y shape: (24594, 253, 40)\n",
            "Saving word indexs  gdrive/My Drive/NLP-CA4/word2index-1-500000.txt\n",
            "Saving word indexs  gdrive/My Drive/NLP-CA4/tag2index-1-500000.txt\n",
            "Fold: 1\n",
            "Preparing data ....\n",
            "Training and testing ...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
            "  % (min_groups, self.n_splits)), Warning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 253, 128)          3951232   \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 253, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 253, 40)           20520     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 253, 40)           0         \n",
            "=================================================================\n",
            "Total params: 4,760,232\n",
            "Trainable params: 4,760,232\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/6\n",
            "19665/19665 [==============================] - 870s 44ms/step - loss: 0.3337 - acc: 0.9419 - ignore_accuracy: 0.3384\n",
            "Epoch 2/6\n",
            " 7808/19665 [==========>...................] - ETA: 8:41 - loss: 0.1638 - acc: 0.9534 - ignore_accuracy: 0.3931"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQF0_Epz5iX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#=================================================== Testing the model\n",
        "\n",
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            m = np.argmax(categorical)\n",
        "            token_sequence.append(index[m])\n",
        "        token_sequences.append(token_sequence) \n",
        "    return token_sequences\n",
        "  \n",
        "# This is a sample file for testing the model\n",
        "testFileName = data_path + '/test.txt'\n",
        "testFile = open(testFileName, 'r', encoding=\"utf-8\")\n",
        "wordList = []\n",
        "for lines in testFile:\n",
        "    wordList.append(lines.split())\n",
        "testFile.close()\n",
        "\n",
        "\n",
        "test_samples, test_tags =extract_sentences(wordList)\n",
        "\n",
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        "\n",
        "\n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "predictions = model.predict(test_samples_X)\n",
        "tagDict = {i: t for t, i in tag2index.items()}\n",
        "\n",
        "y_pred = logits_to_tokens(predictions, tagDict)\n",
        "\n",
        "outFilename = data_path + '/results.txt'\n",
        "outFile = open(outFilename, 'w', encoding=\"utf-8\")\n",
        "\n",
        "y_pred_list = []\n",
        "y_test_list = []\n",
        "k = 1\n",
        "for i in range(len(test_samples)):\n",
        "  yp = y_pred[i]\n",
        "  ya = test_tags[i]\n",
        "  for j,x in enumerate(ya):\n",
        "    y_test_list.append(x)\n",
        "    y_pred_list.append(yp[j])\n",
        "    outFile.writelines(f\"{k}\\t{yp[j]}\\t{x}\\n\")\n",
        "    k+=1\n",
        "\n",
        "    \n",
        "outFile.close()\n",
        "print(\"Number of Tags:\", len(y_test_list))\n",
        "\n",
        "print(\"Actual:\",y_test_list)\n",
        "print(\"Predicted:\",y_pred_list)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "print(classification_report(y_test_list, y_pred_list))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGtEgXAw27CD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def unique(list1):      \n",
        "    # insert the list to the set \n",
        "    list_set = set(list1) \n",
        "    # convert the set to the list \n",
        "    unique_list = (list(list_set)) \n",
        "    return unique_list\n",
        "  \n",
        "y_test = y_test_list\n",
        "y_pred = y_pred_list\n",
        "class_names = sorted(unique(y_test_list))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "# plot_confusion_matrix(y_test_list, y_pred_list, classes=y_test_list,\n",
        "#                       title='Confusion matrix, with normalization')\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "cm = confusion_matrix(y_test, y_pred, class_names)\n",
        "normalize = True\n",
        "format = \"d\"\n",
        "if normalize:\n",
        "    cm = 100*(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])\n",
        "    format = '0.0f'\n",
        "    \n",
        "print(\"Confusion Matrix:\")\n",
        "\n",
        "aylabels = class_names\n",
        "axlabels = class_names\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.title(\"Confusion Matrix\")\n",
        "sn.set(font_scale=1.4)  # for label size\n",
        "sn.heatmap(cm, annot=True,fmt=format, annot_kws={\"size\": 12}, xticklabels=axlabels, yticklabels=aylabels)  # font size\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}